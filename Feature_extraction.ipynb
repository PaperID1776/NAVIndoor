{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73f9378-a502-4406-b0f6-94cc4759df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix# Assuming x is a list of text documents and y is a binary label (0 or 1)\n",
    "# You need to convert text data into numerical features using CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "n_scenes = 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78975e4-959b-470e-a4e7-30f00cb42d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 1048.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#extract RL features\n",
    "model_name = 'RLnv'\n",
    "feature = 'q_values'\n",
    "for n in tqdm(range(n_scenes)):\n",
    "    outputs = torch.stack([results[model_name][n][i][feature] for i in results[model_name][n].keys()]).cpu().detach()\n",
    "    with open(model_name+feature+'/'+str(n)+'.pickle', 'wb') as f:\n",
    "            pickle.dump(outputs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20c1892-312a-4f90-8c55-2e76a8cbe9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:17<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "#extract RL features\n",
    "\n",
    "\n",
    "    \n",
    "distances_Y = []\n",
    "q_values_X = []\n",
    "n_scenes = 13\n",
    "\n",
    "distances_Y_test = []\n",
    "q_values_X_test = []\n",
    "train = np.array([ 4,  9,  8,  5,  6,  3,  0,  1, 12, 10])\n",
    "model_name = 'RLnv'\n",
    "feature = 'q_values'\n",
    "for n in tqdm(range(n_scenes)):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    q_values = torch.stack([results[model_name][n][i][feature] for i in results[model_name][n].keys()]).cpu().detach()\n",
    "    with open('RLnvq_values/'+str(n)+'.pickle', 'wb') as f:\n",
    "            pickle.dump(outputs,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c6d3c0-6ab1-4db4-b3c6-723f44feddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:06<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#save distances\n",
    "\n",
    "\n",
    "n_scenes = 13\n",
    "\n",
    "for n in tqdm(range(n_scenes)):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    with open('distances/'+str(n)+'.pickle', 'wb') as f:\n",
    "            pickle.dump(distances,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a66f04-d5d7-4251-89fb-50e87bc92053",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dino_features.pickle', 'rb') as f:\n",
    "                X,Y,Y_reg,X_test,Y_reg_test, Y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b42e3b-4e73-4ecd-8667-4836e26e7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isarbout/.pyenv/versions/3.10.12/envs/unity/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-02-28 23:33:33.077116: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 23:33:33.078842: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 23:33:33.097019: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 23:33:33.097032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 23:33:33.097051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 23:33:33.101545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 23:33:33.632715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/isarbout/.pyenv/versions/3.10.12/envs/unity/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/isarbout/.pyenv/versions/3.10.12/envs/unity/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:07<00:00, 216.62it/s]\n",
      "100%|█████████████████████████████████████| 1728/1728 [00:00<00:00, 4257.06it/s]\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:14<00:00, 118.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1536/1536 [00:08<00:00, 180.60it/s]\n",
      "100%|█████████████████████████████████████| 1536/1536 [00:00<00:00, 4417.57it/s]\n",
      "100%|██████████████████████████████████████| 1536/1536 [00:13<00:00, 115.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1548/1548 [00:09<00:00, 168.26it/s]\n",
      "100%|█████████████████████████████████████| 1548/1548 [00:00<00:00, 4413.84it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:13<00:00, 116.36it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:07<00:00, 191.60it/s]\n",
      "100%|█████████████████████████████████████| 1500/1500 [00:00<00:00, 4451.41it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:12<00:00, 119.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1320/1320 [00:07<00:00, 186.42it/s]\n",
      "100%|█████████████████████████████████████| 1320/1320 [00:00<00:00, 4448.65it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:11<00:00, 118.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1128/1128 [00:06<00:00, 187.34it/s]\n",
      "100%|█████████████████████████████████████| 1128/1128 [00:00<00:00, 4400.47it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:09<00:00, 119.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1512/1512 [00:07<00:00, 215.64it/s]\n",
      "100%|█████████████████████████████████████| 1512/1512 [00:00<00:00, 4416.29it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:12<00:00, 118.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 972/972 [00:04<00:00, 217.29it/s]\n",
      "100%|███████████████████████████████████████| 972/972 [00:00<00:00, 4393.38it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:08<00:00, 117.77it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:07<00:00, 212.59it/s]\n",
      "100%|█████████████████████████████████████| 1644/1644 [00:00<00:00, 4424.49it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:13<00:00, 118.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1392/1392 [00:06<00:00, 216.87it/s]\n",
      "100%|█████████████████████████████████████| 1392/1392 [00:00<00:00, 4363.23it/s]\n",
      "100%|██████████████████████████████████████| 1392/1392 [00:11<00:00, 117.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1548/1548 [00:07<00:00, 212.70it/s]\n",
      "100%|█████████████████████████████████████| 1548/1548 [00:00<00:00, 4329.97it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:13<00:00, 118.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1224/1224 [00:05<00:00, 208.20it/s]\n",
      "100%|█████████████████████████████████████| 1224/1224 [00:00<00:00, 4280.97it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:10<00:00, 118.43it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:07<00:00, 211.70it/s]\n",
      "100%|█████████████████████████████████████| 1488/1488 [00:00<00:00, 4180.01it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:12<00:00, 118.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing...\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "# Assuming you have a processor that can convert logits to predictions\n",
    "# (e.g., converting logits to segmentation mask using argmax)\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b2-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b2-finetuned-ade-512-512\").to(device)\n",
    "\n",
    "distances_Y = []\n",
    "q_values_X = torch.tensor([])\n",
    "n_scenes = 13\n",
    "\n",
    "distances_Y_test = []\n",
    "q_values_X_test = torch.tensor([])\n",
    "train = np.array([ 4,  9,  8,  5,  6,  3,  0,  1, 12, 10])\n",
    "model_name = 'RLfloor'\n",
    "for n in range(n_scenes):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    \n",
    "    images = np.array([image_dict[i]['img'] for i in image_dict.keys()])\n",
    "    inputs = [processor(images=torch.tensor(image).unsqueeze(0), return_tensors=\"pt\") for image in tqdm(images)]\n",
    "    for input in tqdm(inputs):\n",
    "        input['pixel_values'] = input['pixel_values'].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.stack([model(**input,output_hidden_states=True).hidden_states[3].flatten().cpu().detach() for input in tqdm(inputs)])\n",
    "    with open('segformer_features/'+str(n)+'.pickle', 'wb') as f:\n",
    "                pickle.dump(outputs,f)\n",
    "    if n in train:\n",
    "        print('listing...')\n",
    "        \n",
    "        q_values_X = torch.cat((q_values_X, outputs))\n",
    "        distances_Y.extend(distances)\n",
    "    else:\n",
    "        q_values_X_test = torch.cat((q_values_X_test, outputs))\n",
    "        distances_Y_test.extend(distances)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b33189-e694-4419-9a3e-69322f4dc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isarbout/.pyenv/versions/3.10.12/envs/unity/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-02-28 23:51:22.102238: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 23:51:22.105061: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 23:51:22.128009: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 23:51:22.128031: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 23:51:22.128051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 23:51:22.134875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 23:51:22.639556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/isarbout/.pyenv/versions/3.10.12/envs/unity/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:06<00:00, 257.45it/s]\n",
      "100%|████████████████████████████████████| 1728/1728 [00:00<00:00, 12232.52it/s]\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:07<00:00, 217.11it/s]\n",
      "100%|██████████████████████████████████████| 1536/1536 [00:05<00:00, 288.06it/s]\n",
      "100%|████████████████████████████████████| 1536/1536 [00:00<00:00, 15759.73it/s]\n",
      "100%|██████████████████████████████████████| 1536/1536 [00:06<00:00, 232.36it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:05<00:00, 277.09it/s]\n",
      "100%|████████████████████████████████████| 1548/1548 [00:00<00:00, 15766.68it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:06<00:00, 224.67it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:05<00:00, 275.02it/s]\n",
      "100%|████████████████████████████████████| 1500/1500 [00:00<00:00, 15386.26it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:06<00:00, 226.82it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:04<00:00, 276.82it/s]\n",
      "100%|████████████████████████████████████| 1320/1320 [00:00<00:00, 14313.55it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:05<00:00, 231.33it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:03<00:00, 287.73it/s]\n",
      "100%|████████████████████████████████████| 1128/1128 [00:00<00:00, 14893.18it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:04<00:00, 229.90it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:06<00:00, 236.63it/s]\n",
      "100%|████████████████████████████████████| 1512/1512 [00:00<00:00, 14616.52it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:06<00:00, 231.65it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:03<00:00, 272.26it/s]\n",
      "100%|██████████████████████████████████████| 972/972 [00:00<00:00, 14672.31it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:04<00:00, 231.06it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:06<00:00, 267.84it/s]\n",
      "100%|████████████████████████████████████| 1644/1644 [00:00<00:00, 14796.50it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:07<00:00, 227.80it/s]\n",
      "100%|██████████████████████████████████████| 1392/1392 [00:05<00:00, 276.83it/s]\n",
      "100%|████████████████████████████████████| 1392/1392 [00:00<00:00, 14788.24it/s]\n",
      "100%|██████████████████████████████████████| 1392/1392 [00:06<00:00, 229.61it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:05<00:00, 266.56it/s]\n",
      "100%|████████████████████████████████████| 1548/1548 [00:00<00:00, 14463.76it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:06<00:00, 229.55it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:04<00:00, 290.43it/s]\n",
      "100%|████████████████████████████████████| 1224/1224 [00:00<00:00, 14732.06it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:05<00:00, 230.58it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:06<00:00, 247.27it/s]\n",
      "100%|████████████████████████████████████| 1488/1488 [00:00<00:00, 14603.36it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:06<00:00, 226.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "#compute dino features\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base').to('cuda')\n",
    "\n",
    "distances_Y = []\n",
    "q_values_X = torch.tensor([])\n",
    "n_scenes = 13\n",
    "\n",
    "distances_Y_test = []\n",
    "q_values_X_test = torch.tensor([])\n",
    "train = np.array([ 4,  9,  8,  5,  6,  3,  0,  1, 12, 10])\n",
    "for n in range(n_scenes):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    \n",
    "    images = np.array([image_dict[i]['img'].resize((224,224)) for i in image_dict.keys()])\n",
    "    inputs = [processor(images=torch.tensor(image).unsqueeze(0), return_tensors=\"pt\") for image in tqdm(images)]\n",
    "    for input in tqdm(inputs):\n",
    "        input['pixel_values'] = input['pixel_values'].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.stack([model(**input).last_hidden_state.flatten().cpu().detach() for input in tqdm(inputs)])\n",
    "    with open('dinov2_features/'+str(n)+'.pickle', 'wb') as f:\n",
    "                pickle.dump(outputs,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ffde0d-8587-4d01-a9f5-24002ce02e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1728/1728 [00:14<00:00, 119.68it/s]\n",
      "100%|█████████████████████████████████████| 1728/1728 [00:00<00:00, 3222.40it/s]\n",
      "100%|███████████████████████████████████████| 1728/1728 [00:18<00:00, 93.73it/s]\n",
      "100%|███████████████████████████████████████| 1536/1536 [00:18<00:00, 81.62it/s]\n",
      "100%|█████████████████████████████████████| 1536/1536 [00:00<00:00, 3318.94it/s]\n",
      "100%|███████████████████████████████████████| 1536/1536 [00:16<00:00, 91.49it/s]\n",
      "100%|███████████████████████████████████████| 1548/1548 [00:16<00:00, 91.13it/s]\n",
      "100%|█████████████████████████████████████| 1548/1548 [00:00<00:00, 3318.29it/s]\n",
      "100%|███████████████████████████████████████| 1548/1548 [00:16<00:00, 93.98it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:12<00:00, 117.77it/s]\n",
      "100%|█████████████████████████████████████| 1500/1500 [00:00<00:00, 3386.61it/s]\n",
      "100%|███████████████████████████████████████| 1500/1500 [00:16<00:00, 92.22it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:11<00:00, 115.59it/s]\n",
      "100%|█████████████████████████████████████| 1320/1320 [00:00<00:00, 3417.35it/s]\n",
      "100%|███████████████████████████████████████| 1320/1320 [00:14<00:00, 94.03it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:09<00:00, 115.24it/s]\n",
      "100%|█████████████████████████████████████| 1128/1128 [00:00<00:00, 3407.48it/s]\n",
      "100%|███████████████████████████████████████| 1128/1128 [00:12<00:00, 93.80it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:12<00:00, 119.68it/s]\n",
      "100%|█████████████████████████████████████| 1512/1512 [00:00<00:00, 3402.77it/s]\n",
      "100%|███████████████████████████████████████| 1512/1512 [00:15<00:00, 94.61it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:08<00:00, 114.88it/s]\n",
      "100%|███████████████████████████████████████| 972/972 [00:00<00:00, 3384.73it/s]\n",
      "100%|█████████████████████████████████████████| 972/972 [00:10<00:00, 93.69it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:14<00:00, 115.39it/s]\n",
      "100%|█████████████████████████████████████| 1644/1644 [00:00<00:00, 3393.40it/s]\n",
      "100%|███████████████████████████████████████| 1644/1644 [00:17<00:00, 93.77it/s]\n",
      "100%|███████████████████████████████████████| 1392/1392 [00:15<00:00, 90.45it/s]\n",
      "100%|█████████████████████████████████████| 1392/1392 [00:00<00:00, 3261.72it/s]\n",
      "100%|███████████████████████████████████████| 1392/1392 [00:14<00:00, 93.94it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:13<00:00, 114.80it/s]\n",
      "100%|█████████████████████████████████████| 1548/1548 [00:00<00:00, 3345.43it/s]\n",
      "100%|███████████████████████████████████████| 1548/1548 [00:16<00:00, 91.54it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:10<00:00, 112.34it/s]\n",
      "100%|█████████████████████████████████████| 1224/1224 [00:00<00:00, 3370.21it/s]\n",
      "100%|███████████████████████████████████████| 1224/1224 [00:13<00:00, 92.15it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:13<00:00, 113.08it/s]\n",
      "100%|█████████████████████████████████████| 1488/1488 [00:00<00:00, 3374.37it/s]\n",
      "100%|███████████████████████████████████████| 1488/1488 [00:16<00:00, 92.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "preprocessor = EfficientNetImageProcessor.from_pretrained(\"google/efficientnet-b7\")\n",
    "model = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\").to('cuda')\n",
    "\n",
    "for n in range(n_scenes):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    \n",
    "    images = np.array([image_dict[i]['img'] for i in image_dict.keys()])\n",
    "    inputs = [preprocessor(images=torch.tensor(image).unsqueeze(0), return_tensors=\"pt\") for image in tqdm(images)]\n",
    "    for input in tqdm(inputs):\n",
    "        input['pixel_values'] = input['pixel_values'].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.stack([model(**input,output_hidden_states=True).hidden_states[55].flatten().cpu().detach() for input in tqdm(inputs)])\n",
    "    with open('EfficientNetb7_features/'+str(n)+'.pickle', 'wb') as f:\n",
    "                pickle.dump(outputs,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2cb6cf-70dd-4c78-aea6-ab01a2c072ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████▉           | 1237/1728 [00:37<00:14, 33.05it/s]\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:03<00:00, 442.63it/s]\n",
      "100%|██████████████████████████████████████| 1728/1728 [00:03<00:00, 449.67it/s]\n",
      "100%|██████████████████████████████████████| 1536/1536 [00:03<00:00, 500.61it/s]\n",
      "100%|██████████████████████████████████████| 1536/1536 [00:03<00:00, 454.50it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:03<00:00, 492.66it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:03<00:00, 460.95it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:02<00:00, 509.85it/s]\n",
      "100%|██████████████████████████████████████| 1500/1500 [00:03<00:00, 452.50it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:02<00:00, 505.01it/s]\n",
      "100%|██████████████████████████████████████| 1320/1320 [00:02<00:00, 455.30it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:02<00:00, 487.60it/s]\n",
      "100%|██████████████████████████████████████| 1128/1128 [00:02<00:00, 464.47it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:02<00:00, 511.50it/s]\n",
      "100%|██████████████████████████████████████| 1512/1512 [00:03<00:00, 453.41it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:01<00:00, 510.00it/s]\n",
      "100%|████████████████████████████████████████| 972/972 [00:02<00:00, 460.73it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:03<00:00, 476.90it/s]\n",
      "100%|██████████████████████████████████████| 1644/1644 [00:03<00:00, 457.71it/s]\n",
      "100%|██████████████████████████████████████| 1392/1392 [00:02<00:00, 517.29it/s]\n",
      "100%|██████████████████████████████████████| 1392/1392 [00:03<00:00, 450.78it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:03<00:00, 497.67it/s]\n",
      "100%|██████████████████████████████████████| 1548/1548 [00:03<00:00, 449.89it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:02<00:00, 477.34it/s]\n",
      "100%|██████████████████████████████████████| 1224/1224 [00:02<00:00, 451.71it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:03<00:00, 477.17it/s]\n",
      "100%|██████████████████████████████████████| 1488/1488 [00:03<00:00, 455.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "model = timm.create_model(\n",
    "    'convnextv2_atto.fcmae',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classifier nn.Linear\n",
    ").to('cuda')\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "for n in range(n_scenes):\n",
    "    with open('scene_'+str(n)+'.pickle', 'rb') as f:\n",
    "            data,image_dict = pickle.load(f)\n",
    "    distances = np.array([image_dict[i]['distance'] for i in image_dict.keys()])\n",
    "    \n",
    "    images = [image_dict[i]['img'] for i in image_dict.keys()]\n",
    "    inputs = [transforms(image).to('cuda').unsqueeze(0) for image in tqdm(images)]\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.stack([model.forward_features(input).flatten().cpu().detach() for input in tqdm(inputs)])\n",
    "    with open('ConvNextv2_features/'+str(n)+'.pickle', 'wb') as f:\n",
    "                pickle.dump(outputs,f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
